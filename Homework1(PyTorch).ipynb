{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework1(PyTorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu159KabWLgnN0EwpahpIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaBabushkina/ML_homework/blob/main/Homework1(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABx78K3VEoP0"
      },
      "source": [
        "# /configs/config.py\n",
        "import os\n",
        "\n",
        "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "PATH = os.path.join(ROOT_DIR, \"data/mnist\")\n",
        "FILENAME = 'mnist.pkl'\n",
        "\n",
        "base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "filename = [\n",
        "    [\"training_images\", \"train-images-idx3-ubyte.gz\"],\n",
        "    [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],\n",
        "    [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],\n",
        "    [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "\n",
        "nrof_epochs = 5\n",
        "batch_size = 128\n",
        "lr = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFeeWD_EoDz"
      },
      "source": [
        "# /dataloader/data_transforms.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def normalize_images(images):\n",
        "    return (np.asarray(images, dtype=np.float32) - 127) / 255\n",
        "\n",
        "\n",
        "def convert2tensor(images):\n",
        "    return torch.tensor(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGmEs8bBEnye"
      },
      "source": [
        "# /dataloader/simple_dataloader.py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleDataLoader:\n",
        "    def __init__(self, dataset, config, dataset_type, shuffle=False, transforms=[], skip_last=False):\n",
        "        # преобазования типа данных из numpy в torch.tensor\n",
        "        self.state = 0\n",
        "        self.shuffle = shuffle\n",
        "        self.config = config\n",
        "        self.transforms = transforms\n",
        "        self.skip_last = skip_last\n",
        "        self.images, self.labels = dataset.get_dataset(dataset_type)\n",
        "        self.images, self.labels = map(torch.tensor, (self.images, self.labels))\n",
        "        self.dataset_size = len(self.labels)\n",
        "\n",
        "    def batch_generator(self):\n",
        "        if self.shuffle:\n",
        "            self.images, self.labels = self.shuffle_data(self.images, self.labels)\n",
        "        self.epoch_size = len(self.images) // self.config.batch_size\n",
        "        if not self.skip_last:\n",
        "            self.epoch_size += 1\n",
        "        for i in range(0, self.epoch_size):\n",
        "            start = i * self.config.batch_size\n",
        "            end = (i + 1) * self.config.batch_size\n",
        "            batch_labels = self.labels[start: end]\n",
        "            batch_data = self.images[start: end, :]\n",
        "            for transform in self.transforms:\n",
        "                batch_data = transform(batch_data)\n",
        "            yield (batch_data, batch_labels)\n",
        "\n",
        "    def shuffle_data(self, images, labels):\n",
        "        indexes = np.arange(len(self.labels))\n",
        "        indexes = np.random.permutation(indexes)\n",
        "        images = images[indexes]\n",
        "        labels = labels[indexes]\n",
        "        return images, labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB_7UD32Jljp"
      },
      "source": [
        "# /dataset/mnist_dataset.py\n",
        "from urllib import request\n",
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "class MNIST:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self._read_dataset()\n",
        "\n",
        "    def _read_dataset(self):\n",
        "        if not os.path.exists(os.path.join(self.config.PATH, self.config.FILENAME)):\n",
        "            self._download_dataset()\n",
        "        # считывание данных из pickle файлов; каждое изображение хранится в виде вектора размера 28*28\n",
        "        self.dataset = {}\n",
        "        with open(os.path.join(self.config.PATH, self.config.FILENAME), \"rb\") as f:\n",
        "            data = pickle.load(f, encoding=\"latin-1\")\n",
        "        self.dataset = {'train': {'images': data['training_images'], 'labels': data['training_labels']},\n",
        "                        'test': {'images': data['test_images'], 'labels': data['test_labels']}}\n",
        "\n",
        "    def get_dataset(self, dataset_type):\n",
        "        return self.dataset[dataset_type]['images'], self.dataset[dataset_type]['labels']\n",
        "\n",
        "    def _download_dataset(self):\n",
        "        os.makedirs(self.config.PATH, exist_ok=True)\n",
        "        for name in self.config.filename:\n",
        "            if not os.path.exists(os.path.join(self.config.PATH, name[0])):\n",
        "                print(\"Downloading \" + name[1] + \"...\")\n",
        "                request.urlretrieve(self.config.base_url + name[1], self.config.PATH + name[1])\n",
        "                print(\"Download complete.\")\n",
        "        self._save_mnist()\n",
        "\n",
        "    def _save_mnist(self):\n",
        "        mnist = {}\n",
        "        for name in self.config.filename[:2]:\n",
        "            with gzip.open(self.config.PATH + name[1], 'rb') as f:\n",
        "                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28)\n",
        "        for name in self.config.filename[-2:]:\n",
        "            with gzip.open(self.config.PATH + name[1], 'rb') as f:\n",
        "                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8).astype(int)\n",
        "        with open(os.path.join(self.config.PATH, self.config.FILENAME), 'wb') as f:\n",
        "            pickle.dump(mnist, f)\n",
        "        print(\"Save complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctoEnrK8JlL3"
      },
      "source": [
        "# /executors/train_mnist.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from dataset.mnist_dataset import MNIST\n",
        "from dataloader.simple_dataloader import SimpleDataLoader\n",
        "from nets.simple_net import SimpleNet\n",
        "from metrics.accuracy import accuracy\n",
        "from losses.simple_loss import nll\n",
        "from configs import config\n",
        "from dataloader.data_transforms import normalize_images, convert2tensor\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.dataset = MNIST(config)\n",
        "        self.train_loader = SimpleDataLoader(self.dataset, config, 'train', True, skip_last=True,\n",
        "                                             transforms=[normalize_images, convert2tensor])\n",
        "        self.test_loader = SimpleDataLoader(self.dataset, config, 'test', False,\n",
        "                                            transforms=[normalize_images, convert2tensor])\n",
        "        self.model = SimpleNet(config)\n",
        "\n",
        "    def fit(self):\n",
        "        for epoch in range(self.config.nrof_epochs):\n",
        "            self.train_epoch(epoch)\n",
        "            self.eval(epoch)\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        for i, (batch_data, batch_labels) in enumerate(self.train_loader.batch_generator()):\n",
        "            loss = self.model.train_step(batch_data, batch_labels)\n",
        "            print(f'epoch: [{epoch}/{self.config.nrof_epochs}], '\n",
        "                  f'iter: [{i}/{self.train_loader.epoch_size}], '\n",
        "                  f'loss: {loss}')\n",
        "\n",
        "    def eval(self, epoch):\n",
        "        total_accuracy = []\n",
        "        total_loss = []\n",
        "        with torch.no_grad():\n",
        "            for batch_data, batch_labels in self.test_loader.batch_generator():\n",
        "                predictions = self.model(batch_data)\n",
        "                sum_accuracy = accuracy(predictions, batch_labels).item() * batch_labels.size(0)\n",
        "                total_accuracy.append(sum_accuracy)\n",
        "\n",
        "                sum_loss = nll(predictions, batch_labels).item() * batch_labels.size(0)\n",
        "                total_loss.append(sum_loss)\n",
        "        print(f'epoch: {epoch}, total accuracy: {np.sum(total_accuracy) * 100 / self.test_loader.dataset_size}%, '\n",
        "              f'total loss: {np.sum(total_loss) / self.test_loader.dataset_size}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainer = Trainer(config)\n",
        "    trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxGu0x8IJlIq"
      },
      "source": [
        "# /losses/simple_loss.py\n",
        "\n",
        "# инициализация целевой функции\n",
        "def nll(input, target):\n",
        "    return -input[range(target.shape[0]), target].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYKGZHh0JlEq"
      },
      "source": [
        "# /metrics/accuracy.py\n",
        "import torch\n",
        "\n",
        "\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nijiwuH9JlAQ"
      },
      "source": [
        "# /nets/linear_optim_loss.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb.view(xb.size(0), -1))\n",
        "\n",
        "\n",
        "class SimpleNet:\n",
        "    def __init__(self, config):\n",
        "        self.model = LogReg(config.input_size, config.output_size)\n",
        "        self.optim = torch.optim.SGD(self.model.parameters(), lr=config.lr)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def __call__(self, xb):\n",
        "        return self.model(xb)\n",
        "\n",
        "    def train_step(self, xb, yb):\n",
        "        self.optim.zero_grad()\n",
        "        pred = self(xb)\n",
        "        loss = self.loss_function(pred, yb)\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuPJZvw0Kin7"
      },
      "source": [
        "# /nets/simple_net.py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from losses.simple_loss import nll\n",
        "\n",
        "class SimpleNet:\n",
        "    def __init__(self, config):\n",
        "        self.weights = torch.randn(config.input_size, config.output_size) / np.sqrt(config.input_size)\n",
        "        self.weights.requires_grad_()\n",
        "        self.bias = torch.zeros(config.output_size, requires_grad=True)\n",
        "        self.lr = config.lr\n",
        "\n",
        "    def log_softmax(self, x):\n",
        "        return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "    # forward pass\n",
        "    def __call__(self, xb):\n",
        "        return self.log_softmax(xb @ self.weights + self.bias)\n",
        "\n",
        "    def train_step(self, xb, yb):\n",
        "        pred = self(xb)\n",
        "        loss = nll(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        # обновление градиентов необходимо делать внутри контекста torch.no_grad(),\n",
        "        # чтобы эти действия не повлияли на вычисления градиентов на следующей итерации\n",
        "        with torch.no_grad():\n",
        "            self.weights -= self.weights.grad * self.lr\n",
        "            self.bias -= self.bias.grad * self.lr\n",
        "            # Затем мы устанавливаем значения градиентов равными нулю.\n",
        "            # В противном случае наши градиенты будут суммироваться с градиентами с прошлых итераций\n",
        "            # (т.е. loss.backward () добавляет градиенты к тому, что уже сохранено, а не заменяет их).\n",
        "            self.weights.grad.zero_()\n",
        "            self.bias.grad.zero_()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl0rALkIKieS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBFsydRyKiTB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}